---
layout: default
---

<div id="container">
<span class="sup">Vasiliy Karasev</span>
<a href="mailto:karasev00@gmail.com"><img src="data/email.png" class="logoimage" alt="email"/></a>
<a href="https://github.com/vasiliykarasev"><img src="data/github.png" class="logoimage" alt="github"/></a>
</div>

<div id="container">

<img style="float: left; margin: 5px 15px 5px 0px;" src="data/v.jpg" title="death valley, ca" height="250" />
<!--

<p class=nomargin>
<a href="http://ee.ucla.edu">Department of Electrical Engineering<br>
University of California, Los Angeles</a><br>
Boelter Hall, #3811<br>
405 Hilgard Ave, Los Angeles, CA 90095<br>
email: karasev00 at gmail dot com
</p>
<p>
-->

I am a senior software engineer on the computer vision team at <a href="http://zoox.com/">Zoox</a>.<br><br>
Before graduating from UCLA in Fall 2015, I was a PhD student in <a href="http://ee.ucla.edu">Electrical Engineering</a>, where I worked with <a href="http://vision.ucla.edu/">Prof. Stefano Soatto</a>.
My interests are in ''value of information'' problems, optimization, and their applications in computer vision. During my studies I had the opportunity to visit <a href="http://honda-ri.com/">Honda Research Institute</a> and <a href="https://www.samsungdisplay.com/">Samsung Display</a> labs. <br>
Prior to that (in reverse chronological order) I worked with <a href="http://ee.ucla.edu/~jhlgroup">Prof. Jin Hyung Lee</a> (UCLA EE),  <a href="http://www-video.eecs.berkeley.edu/~avz">Prof. Avideh Zakhor</a> (UCB EECS), and  <a href="http://vision.berkeley.edu/?p=369">Prof. Martin Banks</a> (UCB Vision Science).</p>

<p>
email: karasev00 at gmail dot com

</div>

<div id="container">
<h3> Projects </h3>

<!-- project -->
<div id="proj">
<img class=thumbnail src="data/icra16.png" height="100" />
<p class=nomargin>
<span class=projtitle>Intent-Aware Long-Term Prediction of Pedestrian Motion</span>
<span class=projauthors>V. Karasev, A. Ayvaci, B. Heisele, and S. Soatto.</span>
<span class=projvenue>In ICRA 2016.</span>
[<a href="http://vision.ucla.edu/papers/karasevAHS16.pdf">pdf</a>]
[<a href="https://vimeo.com/157014891">video</a>]

<br><br><span class=projdesc>Forecasting what pedestrians intend to do is easier if they behave rationally. We show how this assumption simplifies motion prediction in the assisted/autonomous driving setting.</span>
</p>
</div>


<!-- project -->
<div id="proj">
<img class=thumbnail src="data/cvpr15.png" height="100" />
<p class=nomargin>
<span class=projtitle>Causal Video Object Segmentation from Persistence of Occlusions</span>
<span class=projauthors>B. Taylor, V. Karasev, and S. Soatto.</span>
<span class=projvenue>In CVPR 2015.</span>
[<a href="http://vision.ucla.edu/papers/taylorKS15.pdf">pdf</a>]
[<a href="http://vision.ucla.edu/cvos">project page</a>]

<br><br><span class=projdesc>We show how to exploit occlusions to discover salient objects in video.</span>
</p>
</div>

<!-- project -->
<div id="proj">
<img class=thumbnail src="data/cvpr14.png" height="100" />
<p class=nomargin>
<span class=projtitle>Active Frame, Location, and Detector Selection for Automated and Manual Video Annotation</span>
<span class=projauthors>V. Karasev, A. Ravichandran, and S. Soatto.</span>
<span class=projvenue>In CVPR 2014.</span>
[<a href="http://vision.ucla.edu/papers/karasevRS14.pdf">pdf</a>]
[<a href="http://vision.ucla.edu/activeselection">project page</a>]

<br><br><span class=projdesc>How to choose where and which detectors to run (or m-turks to query), if we can run only a few of them? We answer this question using the ''information gathering'' framework, and show results on semantic video segmentation. </span>
</p>
</div>
<!-- project -->
<div id="proj">
<img class=thumbnail src="data/nips12.png" height="100" />
<p class=nomargin>
<span class=projtitle>Controlled Recognition Bounds for Visual Learning and Exploration</span>
<span class=projauthors>V. Karasev, A. Chiuso, and S. Soatto.</span>
<span class=projvenue>In NIPS 2012.</span>
[<a href="http://vision.ucla.edu/papers/karasevCS12.pdf">pdf</a>]

<br><br><span class=projdesc>We show how to (greedily) search for an unknown object, under occlusions, quantization-scale, and uncertain measurements. </span>
</p>
</div>
<!-- project -->
<div id="proj">
<img class=thumbnail src="data/ms11.jpg" height="100" />
<p class=nomargin>
<span class=projtitle>Compressed sensing enabled ultra-high resolution optogenetic functional magnetic resonance imaging (ofMRI)</span>
<span class=projauthors>J.H. Lee, J. Li, V. Karasev. </span>
<span class=projvenue>In Society for Neuroscience Meeting, 2011.</span>
[<a href="http://www.sfn.org/am2011/pdf/prelim/Nanosymposium_v2.pdf">pdf</a>]

<br><br><span class=projdesc>Compressed sensing in dynamic MRI is normally used to improve temporal resolution. Here we used it to improve <em>spatial</em> resolution. Reconstruction used the simplest possible TV regularization. This work was also a part of my <a href="http://karasev00.bol.ucla.edu/research/ms.html">MS project</a>. </span>
</p>
</div>
<!-- project -->
<div id="proj">
<img class=thumbnail src="data/jsid11.png" height="100" />
<p class=nomargin>
<span class=projtitle>Temporal presentation protocols in stereoscopic displays: Flicker visibility, perceived motion, and perceived depth</span>
<span class=projauthors>D. Hoffman, V.Karasev, and M. Banks. </span>
<span class=projvenue>In Journal of the Society for Information Display, 2011.</span>
[<a href="http://onlinelibrary.wiley.com/doi/10.1889/JSID19.3.271/abstract">pdf</a>]

<br><br><span class=projdesc>We studied how different 3D display presentation methods affect flicker, motion artifacts, and errors in perceived depth.</span>
</p>
</div>

</div>



